# Tecnologia digitale

Miliardi di persone in tutto il mondo portano in giro smartphone, potenti computer collegati a una rete globale (Internet). Spesso passiamo molte ore al giorno su questi dispositivi, giocando o lavorando. Eppure, nonostante la crescente ubiquità della tecnologia digitale, la gente spesso trova difficile capire cosa esattamente la renda così distintamente potente. Alcuni hanno persino deriso la tecnologia digitale, indicando servizi come Twitter e sostenendo che sono irrilevanti se paragonati, per esempio, all'invenzione dei vaccini. 

Tuttavia sta diventando sempre più difficile ignorare la dirompenza della tecnologia digitale. Per esempio, mentre molti settori economici già affermati da tempo sono in difficoltà, tra cui giornali e rivenditori, le società di tecnologia digitale come Facebook, Apple, Amazon, Netflix e Google sono ora tra le più quotate al mondo ("Lista delle società pubbliche", 2020). 

La tecnologia digitale risulta possedere due caratteristiche uniche che spiegano perché espande drammaticamente lo "spazio del possibile" per l'umanità, andando ben oltre tutto ciò che era possibile in precedenza. Queste sono il *costo marginale zero* e l'*universalità del calcolo*. 

## Costo marginale zero 

Una volta che un'informazione esiste su Internet, è possibile accedervi da qualsiasi punto della rete senza costi aggiuntivi. E poiché sempre più persone in tutto il mondo sono connesse a Internet, "ovunque sulla rete" significa sempre più "ovunque nel mondo". I server sono già in funzione, così come le connessioni di rete e i dispositivi degli utenti finali. Fare una copia digitale in più dell'informazione e consegnarla attraverso la rete non costa quindi nulla. Nel linguaggio dell'economia, il "costo marginale" di una copia digitale è zero. Questo non significa che la gente non cercherà di farvi pagare per queste informazioni - in molti casi lo farà. È una questione di costo, non di prezzo.
 
Il costo marginale zero è radicalmente diverso da tutto ciò che è venuto prima nel mondo analogico, e rende possibili alcune cose piuttosto sorprendenti. Per illustrare questo, immaginate di possedere una pizzeria. Pagate l'affitto per il negozio e le attrezzature, e poi dovete pagare gli stipendi per lo staff e per voi stessi. Questi sono i cosiddetti 'costi fissi', e non cambiano con il numero di pizze che prepari. I "costi variabili", invece, dipendono dal numero di pizze che fai. Per una pizzeria, questi includono il costo dell'acqua, della farina, di qualsiasi altro ingrediente usato, di qualsiasi lavoratore aggiuntivo che devi assumere, e dell'energia necessaria per riscaldare il forno. Se fai più pizze, i tuoi costi variabili salgono, e se fai meno pizze scendono. 

Quindi cos'è il costo marginale? Bene, diciamo che stai facendo cento pizze ogni giorno: il costo marginale è il costo aggiuntivo di fare una pizza in più. Supponendo che il forno sia già caldo e abbia spazio, e che i vostri impiegati non siano completamente occupati, è il costo degli ingredienti, che probabilmente è relativamente basso. Se il forno si fosse già raffreddato, allora il costo marginale della pizza in più includerebbe il costo energetico richiesto per riscaldare il forno e potrebbe essere piuttosto alto. 

Da un punto di vista economico, vorreste fare quella pizza aggiuntiva finché potete venderla a più del suo costo marginale. Se avete già coperto i costi fissi delle pizze precedenti, ogni centesimo sopra il costo marginale per la pizza aggiuntiva sarebbe profitto. Il costo marginale conta anche da una prospettiva sociale. Finché un cliente è disposto a pagare più del costo marginale per quella pizza, tutti stanno potenzialmente meglio: voi ottenete un contributo extra che riduce i costi fissi o aumenta i tuoi profitti, e il vostro cliente riesce a mangiare la pizza che voleva (nota importante: sto dicendo "potenzialmente meglio" per una ragione, perché le persone a volte vogliono cose che potrebbero non essere effettivamente buone per loro, come qualcuno che soffre di obesità che vuole mangiare una pizza).

Ora consideriamo cosa succede quando il costo marginale scende da un livello alto. Immaginate che il vostro ingrediente chiave sia un tartufo estremamente costoso, il che significa che il costo marginale di ciascuna delle vostre pizze è di 1.000 dollari. Chiaramente non vendereste molte pizze, quindi potreste decidere di passare a ingredienti più economici e ridurre il vostro costo marginale fino a un punto in cui un maggior numero di clienti è disposto a pagare più del vostro costo marginale, quindi le vostre vendite aumentano. E man mano che abbassate ulteriormente il costo marginale attraverso ulteriori miglioramenti del processo e del prodotto, inizierete a vendere ancora più pizze. 

Ora immagina che attraverso una nuova magica invenzione tu possa fare altre gustose pizze a un costo marginale vicino allo zero (diciamo un centesimo per ogni pizza aggiuntiva) e spedirle istantaneamente in qualsiasi parte del mondo. Sareste quindi in grado di vendere un numero estremamente grande di pizze. Se facessi pagare solo due centesimi per pizza, faresti un centesimo di profitto per ogni pizza in più che vendi. Con un costo marginale così basso probabilmente guadagnereste rapidamente un monopolio sul mercato globale della pizza (più avanti su questo). Chiunque nel mondo avesse fame e potesse permettersi almeno un centesimo potrebbe comprare una delle tue pizze. Il miglior prezzo della vostra pizza da un punto di vista sociale sarebbe un centesimo (il vostro costo marginale): gli affamati sarebbero sfamati, e voi avrete coperto il vostro costo marginale. Ma come monopolisti è improbabile che facciate una cosa simile. Invece, probabilmente vi impegnereste in ogni sorta di comportamento problematico volto ad aumentare i profitti, come far pagare più del costo marginale, cercare di impedire ai concorrenti di entrare nel mercato, e persino cercare di rendere le persone dipendenti dalla pizza in modo che ne consumino sempre di più.

Questo è esattamente dove siamo attualmente con la tecnologia digitale. Possiamo "nutrire il mondo" con le informazioni: quella visualizzazione aggiuntiva del video di YouTube, l'accesso aggiuntivo a Wikipedia, o il rapporto aggiuntivo sul traffico da Waze hanno tutti un costo marginale zero. E proprio come nel caso dell'ipotetica pizza a costo marginale zero, stiamo assistendo all'emergere di monopoli digitali, insieme a tutti i problemi che ne derivano (vedi la quarta parte sulla "libertà informativa" per una proposta di rimedio).

Non siamo abituati al costo marginale zero: la maggior parte della nostra economia attuale dipende dall'assunzione che i costi marginali siano maggiori di zero. Si può pensare al costo marginale zero come a una singolarità economica simile alla divisione per zero in matematica: man mano che ci si avvicina, cominciano ad accadere cose strane. Oltre ai quasi-monopoli digitali, stiamo già osservando distribuzioni di reddito e ricchezza secondo la legge della potenza (vedi la terza parte), dove piccole variazioni portano a risultati enormemente diversi. Inoltre, ci stiamo avvicinando rapidamente a questa singolarità del costo marginale zero in molte altre industrie, che sono principalmente basate sull'informazione, incluse la finanza e l'istruzione. In sintesi, la prima caratteristica della tecnologia digitale che espande drammaticamente lo spazio del possibile è il costo marginale zero. Questo può portare a monopoli digitali, ma ha anche il potenziale per garantire a tutta l'umanità l'accesso alla conoscenza del mondo. 

 
## Universality of Computation 

Zero marginal cost is only one property of digital technology that dramatically expands the space of the possible; the second is in some ways even more amazing.
 
Computers are universal machines. I use this term in a precise sense: anything that can be computed in the universe can in principle be computed by the kind of machine that we already have, given enough memory and time. We have known this since Alan Turing’s groundbreaking work on computation in the middle of the last century. He invented an abstract version of a computer that we now call a Turing machine, before coming up with a proof to show that this simple machine could compute anything (Mullins, 2012; “Church–Turing thesis,” 2020). 

By “computation,” I mean any process that takes information inputs, executes a series of processing steps, and produces information outputs. That is—for better or worse—also much of what a human brain does: it receives inputs via nerves, carries out some internal processing and produces outputs. In principle, a digital machine can accomplish every computation that a human brain can. Those brain computations include something as simple and everyday as recognizing someone’s face (inputs: image, output: name) to something as complicated as diagnosing disease (inputs: symptoms and test results, output: differential diagnosis).

This ‘in principle’ limitation will turn out to be significant only if quantum effects matter for the functioning of the brain, meaning effects that require quantum phenomena such as entanglement and the superposition of states. This is a hotly debated topic (Jedlicka, 2017). Quantum effects do not change what can be computed in principle, as even a Turing machine can theoretically simulate a quantum effect—but it would take an impractically long time, potentially millions of years, to do so (Timpson, 2004). If quantum effects are important in the brain, we may need further progress in quantum computing to replicate some of the brain’s computational capabilities. However, I believe that quantum effects are unlikely to matter for the bulk of computations carried out by the human brain—that is, if they matter at all. We may, of course, one day discover something new about physical reality that will change our view of what is computable, but so far this hasn’t happened. 

For a long time, this property of universality didn’t matter much because computers were pretty dumb compared to humans. This was frustrating to computer scientists who since Turing had believed that it should be possible to build an intelligent machine, but for decades couldn’t get it to work. Even something that humans find really simple, such as recognizing faces, had computers stumped. Now, however, we have computers that can recognize faces, and their performance at doing so is improving rapidly. 

An analogy here is the human discovery of heavier-than-air flight. We knew for a long time that it must be possible—after all, birds are heavier than air and they can fly—but it took until 1903, when the Wright brothers built the first successful airplane, for us to figure out how to do it (“Wright Brothers,” 2020). Once they and several other people had figured it out, progress was rapid—we went from not knowing how to fly to crossing the Atlantic in passenger jet planes in fifty-five years: the British Overseas Airways Corporation’s first transatlantic jet passenger flight was in 1958 (“British Overseas Airways Corporation,” 2020). If you plot this on a graph, you see a perfect example of a non-linearity. We didn’t get gradually better at flying—we couldn’t do it at all, and then suddenly we could do it very well.

![Non-Commercial Flight Distance Records](assets/flight-distance.png)

Digital technology is similar. A series of breakthroughs have taken us from having essentially no machine intelligence to a situation where machines can outperform humans on many different tasks, including reading handwriting and recognizing faces (Neuroscience News, 2018; Phillips et al., 2018). The rate of machines’ progress in learning how to drive cars is another great example of the non-linearity of improvement. The Defense Advanced Research Projects Agency (DARPA) held its first so-called “Grand Challenge” for self-driving cars in 2004. At the time they picked a 150-mile-long closed course in the Mojave Desert, and no car got further than seven miles (less than 5 per cent of the course) before getting stuck. By 2012, less than a decade later, Google’s self-driving cars had driven over 300,000 miles on public roads, with traffic (Urmson, 2012). 

Some people may object that reading handwriting, recognizing faces, or driving a car is not what we mean by ‘intelligence’, but this just points out that we don’t have a good definition of it. After all, if you had a pet dog that could perform any of these tasks, let alone all three, you would call it an ‘intelligent’ dog. 

Other people point out that humans also have creativity and that these machines won’t be creative even if we grant them some form of intelligence. However, this amounts to arguing that creativity is something other than computation. The word implies ‘something from nothing’ and outputs without inputs, but that is not the nature of human creativity. After all, musicians create new music after hearing lots of music, engineers create new machines after seeing existing ones, and so on. 

There is now evidence that at least some types of creativity can be recreated simply through computation. In 2016, Google achieved a breakthrough in machine intelligence when their AlphaGo program beat the South Korean Go grandmaster Lee Sedol by four games to one (Borowiec, 2017). Until that point, progress with game-playing software had been comparatively slow and the best programs were unable to beat strong club players, let alone grandmasters. The number of possible plays in Go is extremely large, far exceeding chess. This means that searching through possible moves and counter-moves from a current position, which is the approach historically used by chess computers, cannot be used in Go—instead, candidate moves need to be conjectured. Put differently, playing Go involves creativity. 

The approach used for the AlphaGo program started out by training a neural network on games previously played by humans. Once the network was good enough, it was improved further by playing against itself. There has already been progress in the application of these and related techniques, which are often referred to as ‘generative adversarial networks’ (GANs) to the composition of music and the creation of designs. Even more surprisingly, it has been shown that machines can learn to be creative not just by studying prior human games or designs, but by creating their own, based on rules. Each of AlphaGo’s two successors, AlphaGo Zero and AlphaZero, started out knowing only the rules and learned from playing games against itself (“AlphaZero,” 2020). This approach will allow machines to be creative in areas where there is limited or no prior human progress.

While much of what the brain does is computation, including many tasks that we identify as creative, there is one function of the brain that may never be accessible to digital machines: having ‘qualia.’ This is a term from philosophy which refers to our subjective experience, such as what it “feels like” to be cold (or hot), to touch an object, be stressed or amazed. For example, when a digital thermostat displays the room temperature we do not assume that its internal state has anything remotely resembling our own subjective sensation. The lack of qualia is obvious in this example, but we assume that it extends to much more complex situations, such as a self-driving car taking a series of turns on a winding highway.  We would expect a human driver to experience a sensation of thrill or elation, but not the car. This lack of qualia in machines may seem like an aside for the moment, but will turn out to be an important component of where humans might direct their attention in the Knowledge Age. 


## Universality at Zero Marginal Cost 

As impressive as zero marginal cost and universality are on their own, in combination they are truly magical. To take one example, we are making good progress in the development of a computer program that will be able to diagnose disease from a patient’s symptoms in a series of steps, including ordering tests and interpreting their results (Parkin, 2020). Though we might have expected this to happen at some point based on the principle of universality, we are making tangible progress and should accomplish this in a matter of decades, if not sooner. At that point, thanks to zero marginal cost, we will be able to provide low-cost diagnosis to anyone in the world. Let that sink in slowly: free medical diagnosis for all humans will soon be in the space of the possible.
 
The universality of computation at zero marginal cost is unlike anything we have had with prior technologies. Being able to make all the world’s information and knowledge accessible to all of humanity was never before possible, nor were intelligent machines. Now we have both. This represents at least as dramatic and non-linear an increase the ‘space of the possible’ for humanity as agriculture and industry did before, and each of those developments ushered in an entirely different age. We will be able to think better about what this implies for the current transition and the next age if we first put some foundations in place.
